---
jupyter: ds-flood-gfm
---

## Cameroon Flood Snapshots

```{python}
import pystac_client
import stackstac
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap, BoundaryNorm
import numpy as np
import geopandas as gpd
from fsspec.implementations.http import HTTPFileSystem
from ds_flood_gfm.geo_utils import load_adm0_lowres
import time

import ocha_stratus as stratus
from datetime import datetime, timedelta

# Configure matplotlib for better display
%matplotlib inline
plt.rcParams['figure.dpi'] = 100

```


## Area of Interest

Define spatial and temporal range for flood analysis. We'll use Admin 1 pcode CM004 in Cameroon for November 2024.
```{python}
GLOBAL_ADM1 = (
    "https://data.fieldmaps.io/edge-matched/humanitarian/intl/adm1_polygons.parquet"
)
ISO3 = "CMR"
filesystem = HTTPFileSystem()
filters = [("iso_3", "=", ISO3)]
gdf = gpd.read_parquet(GLOBAL_ADM1, filesystem=filesystem, filters=filters)
gdf_aoi = gdf[gdf.adm1_src == "CM004"]
bbox = gdf_aoi.total_bounds
```

```{python}
gdf_world_adm0 = load_adm0_lowres()
gdf_subset_admo = gdf_world_adm0[gdf_world_adm0.name == "Cameroon"]
```


## STAC - connect & query
```{python}
# | cache: true
# | cache-comments: "Cache STAC search results - change dates to invalidate"

stac_api = "https://stac.eodc.eu/api/v1"
client = pystac_client.Client.open(stac_api)


start_date = "2024-11-13"
end_date = "2024-11-30"


start_date = "2024-10-14"
end_date = "2024-10-30"
# Step 2: Search for GFM items
datetime_range = f"{start_date}/{end_date}"
search = client.search(
    collections=["GFM"],
    bbox=gdf_aoi.total_bounds,
    datetime=datetime_range,
)

item_collection = search.item_collection()


```




```{python}
#| cache: true
#| cache-lazy: true

stack = stackstac.stack(item_collection, epsg=4326)
```


## Processing

Create daily flood composites using forward-fill to handle temporal gaps.

```{python}
# | cache: true
# | cache-lazy: true

stack_flood = stack.sel(band="ensemble_flood_extent")

stack_flood_clipped = stack_flood.sel(
    x=slice(bbox[0], bbox[2]), y=slice(bbox[3], bbox[1])  # y is reversed
)

# Group by day and take the maximum flood value for each day
stack_flood_max = stack_flood_clipped.resample(time="1D").max()

# Now do forward fill on daily data
ic_latest = stack_flood_max.ffill(dim="time")


start_date_extract = "2024-11-23"
# Create a vector of dates: start_date_extract + 1, +2, +3, +4 days
base_date = datetime.strptime(start_date_extract, "%Y-%m-%d")
date_vector = [base_date + timedelta(days=i) for i in range(1, 5)]
date_strings = [date.strftime("%Y-%m-%d") for date in date_vector]


# Subset ic_latest to the specific dates
ic_latest_subset = ic_latest.sel(time=date_strings)
print(f"Size in memory: {ic_latest_subset.sel(time=date_strings[0]).nbytes / 1024**2:.1f} MB")


def print_mb(da):
    print(f"Size in memory: {da.nbytes / 1024**2:.1f} MB")


```


```{python}


# Create a mask showing which dates have real data (not NaN)
has_data_mask = ~stack_flood_max.isel(x=0, y=0).isnull()  # Just check one pixel

stack_flood_max_dates = stack_flood_max.time.compute()
dates_with_data = stack_flood_max_dates[has_data_mask.values].values

print("Dates with actual observations:")
for date in dates_with_data:
    print(f"  {str(date)[:10]}")

# THEN do forward fill
ic_latest = stack_flood_max.ffill(dim="time")

# Track provenance - which source date does each filled date come from
def get_source_date(time_idx):
    """Find the most recent actual observation date for a given time index"""
    for i in range(time_idx, -1, -1):
        if has_data_mask[i]:
            return stack_flood_max.time[i].values
    return None

# In your loop, track provenance
for time_step in ic_latest_subset.time:
    time_idx = np.where(ic_latest.time == time_step)[0][0]
    source_date = get_source_date(time_idx)
    
    print(f"\nDate: {str(time_step.values)[:10]}")
    print(f"  Source data from: {str(source_date)[:10]}")
    print(f"  Days since last observation: {(time_step - source_date).values.astype('timedelta64[D]').astype(int)}")
```


```{python}
import xarray as xr
# Create a "days since observation" layer
days_since_obs = xr.full_like(stack_flood_max, 0)

for i, time in enumerate(stack_flood_max.time):
    if not has_data_mask[i]:
        # Count days back to last observation
        days_back = 0
        for j in range(i-1, -1, -1):
            days_back += 1
            if has_data_mask[j]:
                break
        days_since_obs[i] = days_back

# Now you can filter or flag old data
ic_latest_fresh = ic_latest.where(days_since_obs <= 3)  # On
```

## Population Data Overlay


Load population for area
```{python}

blob_name = "ghsl/pop/GHS_POP_E2025_GLOBE_R2023A_4326_3ss_V1_0.tif"
da_global = stratus.open_blob_cog(blob_name, container_name="raster").squeeze(drop=True)

# Ensure the global data has the correct CRS
# da_global = da_global.rio.write_crs("EPSG:4326")
da_global.rio.crs

# clip to box (need to do this first, otherwise Python crashes on normal .rio.clip)
min_x, min_y, max_x, max_y = gdf_aoi.total_bounds
da_clip_box = da_global.rio.clip_box(minx=min_x, miny=min_y, maxx=max_x, maxy=max_y)

# clip to admin and preserve CRS
da_clip = da_clip_box.rio.clip(gdf_aoi.geometry)
da_clip = da_clip.rio.write_crs("EPSG:4326")
```

```{python}
print(f"da_clip fill value: {da_clip.rio.nodata}")
print(f"da_clip has NaNs: {da_clip.isnull().any().values}")
print(f"da_clip min (should be 0): {da_clip.min().values}")

# Or more robust - use the nodata value
da_clip = da_clip.where(da_clip != da_clip.rio.nodata)

# Verify
print(f"da_clip min after masking: {da_clip.min().values}")  # Should be 0 or small positive
print(f"da_clip has NaNs: {da_clip.isnull().any().values}")  # Should be True
```

```{python}
# Create a map of the clipped population data
fig, ax = plt.subplots(1, 1, figsize=(12, 8))

# Plot the population data
im = da_clip.plot(
    ax=ax,
    cmap='YlOrRd',
    vmin=0,
    vmax=200,
    add_colorbar=False
)

# Add colorbar
cbar = plt.colorbar(im, ax=ax, label='Population Count')

# Add AOI boundary
gdf_aoi.boundary.plot(ax=ax, color='black', linewidth=2, alpha=1.0)

# Set title and labels
ax.set_title('Population Density 2025 - Clipped to AOI', fontsize=12, fontweight='bold')
ax.set_xlabel('Longitude')
ax.set_ylabel('Latitude')

plt.tight_layout()
plt.show()
```

```{python}
# | eval: false

# NOT CURRENTLY USNG THIS
ic_subset_ready = ic_latest_subset.compute()

print(ic_latest_subset)
print(f"Shape: {ic_latest_subset.shape}")
print(f"Size in memory: {ic_latest_subset.nbytes / 1024**2:.1f} MB")
```
```{python}
# should i clip the gfm stac first?
# Convert dates_with_data to datetime objects

dates_with_data = [pd.Timestamp(date).to_pydatetime() for date in dates_with_data]

# Convert to date strings in one step
date_strings = [date.strftime("%Y-%m-%d") for date in dates_with_data]

# Subset ic_latest to the specific dates
ic_latest_subset_filt = ic_latest.sel(time=date_strings)
# Create binary flood masks and calculate population impacts for each time step
population_impacts = []
dates = []

for i, time_step in enumerate(ic_latest_subset_filt.time):
    # Get flood data for this time step
    flood_data = ic_latest_subset_filt.sel(time=time_step)

    # Create binary flood mask (1 = flooded, 0 = not flooded)
    # Assuming flood values > 0.5 indicate flooding
    flood_binary = (flood_data > 0.5).astype(int)
    flood_binary = flood_binary.rio.write_crs(flood_data.rio.crs)

    # Resample flood data to match population grid resolution
    # Population data is at higher resolution, so we need to align grids
    flood_resampled = flood_binary.rio.reproject_match(
        da_clip, resampling=8
    )  # nearest neighbor

    # Calculate flooded population (multiply binary flood by population)
    pop_flooded = flood_resampled * da_clip

    print(f"Pop flooded valid (non-NaN) cells: {(~pop_flooded.isnull()).sum().values}")
    print(f"Pop flooded max value: {pop_flooded.max().values}")
    print(f"Pop flooded sum (skipna=True): {pop_flooded.sum(skipna=True).values}")
    print(f"Pop flooded sum (skipna=False): {pop_flooded.sum(skipna=False).values}")

    # Store results
    population_impacts.append(pop_flooded)
    dates.append(str(time_step.values)[:10])

    print(f"Date: {dates[i]}")
    print(f"  Total flooded cells: {flood_resampled.sum().values:,.0f}")
    print(f"  Population in flooded areas: {pop_flooded.sum().values:,.0f}")

# Calculate zonal statistics (total flooded population per admin area)
zonal_stats = []
for i, pop_impact in enumerate(population_impacts):
    # Sum population within the admin boundary
    total_pop_flooded = pop_impact.sum().values

    zonal_stats.append(
        {
            "date": dates[i],
            "total_population_flooded": total_pop_flooded,
            "admin_code": gdf_aoi.iloc[0]["adm1_src"],
        }
    )

# Display results
import pandas as pd

df_results = pd.DataFrame(zonal_stats)
print("\n=== Zonal Statistics Summary ===")
print(df_results)
```


Select time slices to visualize the compositing process:

```{python}
# | eval: false


# Get 4 Image snap-shots from STAC for visualization of concept
limg_tstart = ic_latest.isel(time=0)
limg_tearly = ic_latest.isel(time=5)
limg_tmid = ic_latest.isel(time=10)
limg_tlater = ic_latest.isel(time=-1)


```


Pre-compute images for easier plot development.
```{python}
# | eval" false
# | cache: true
# | cache-comments: "Cache computed images to avoid recomputation on plot tweaks"

# this will take a couple minutes

img_tstart = limg_tstart.compute()
img_tearly = limg_tearly.compute()
img_tmid = limg_tmid.compute()
img_tlater = limg_tlater.compute()
```



### Visualize

Plot time slices showing how flood coverage evolves and geographic gaps get filled over time.

```{python}

# Define colors for flood data visualization
colors = ["lightgrey", "darkblue", "white"]  # 0=no flood, 1=flood, 255=nodata

# compute_start = time.time()
cmap = ListedColormap(colors)
bounds = [0, 0.5, 1.5, 255.5]
norm = BoundaryNorm(bounds, cmap.N)

# Create 2x2 subplot grid
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))

# Define image data and metadata for loop processing
images_data = [
    {"img": img_tstart, "ax": ax1, "title": "Start Time - Flood Detection"},
    {"img": img_tearly, "ax": ax2, "title": "Early Time - Flood Detection"},
    {"img": img_tmid, "ax": ax3, "title": "Mid Time - Flood Detection"},
    {"img": img_tlater, "ax": ax4, "title": "Later Time - Flood Detection"},
]

# Process all images in a loop to avoid code repetition
processed_data = []
for i, img_info in enumerate(images_data):
    img_data = img_info["img"]
    ax = img_info["ax"]
    title = img_info["title"]

    # Process data (handle NaN and downsample)
    data_clean = np.where(np.isnan(img_data), 255, img_data)
    data_clean = data_clean[::8, ::8]  # Downsample for speed
    processed_data.append(data_clean)

    # Get extent from the xarray data
    extent = [
        img_data.x.min().item(),
        img_data.x.max().item(),
        img_data.y.min().item(),
        img_data.y.max().item(),
    ]

    # Plot the image
    im = ax.imshow(
        data_clean,
        cmap=cmap,
        norm=norm,
        interpolation="nearest",
        origin="upper",
        extent=extent,
    )

    # Add AOI boundary
    gdf_aoi.boundary.plot(ax=ax, color="black", linewidth=4, alpha=1.0)

    # Get the actual date from the image for the title
    img_date = img_data.time.values
    # Show just the date since we're now working with daily data
    date_str = str(img_date)[:10]  # Extract YYYY-MM-DD format

    # Set title with date and labels
    ax.set_title(f"{title}\n{date_str}", fontsize=12, fontweight="bold")
    ax.set_xlabel("Longitude")
    ax.set_ylabel("Latitude")

# total_compute = time.time() - compute_start
# print(f"âœ… 2x2 flood detection plots completed in {total_compute:.1f} seconds")
for i, data in enumerate(processed_data):
    img_name = ["Start", "Early", "Mid", "Later"][i]
    print(f"{img_name} plot: {data.size:,} pixels rendered")

plt.tight_layout()
plt.show()
# Remove 'fig' to prevent duplicate display
```



### Appendix  - GEE GHSL STuff
Load GHSL population data from Earth Engine and visualize it alongside flood data.

```{python}
import ee
import geemap

# Initialize Earth Engine
ee.Initialize()
```

```{python}
# | cache: true
# | cache-lazy: true

# Download GHSL 2025 population data from Earth Engine
# NOTE: EE's computePixels API has a 50MB limit. This AOI at 100m = 67MB (too large)
# Using 200m resolution = 17MB (works fine). For 100m you'd need to use Task API + Drive export.
pop_2025_ee = ee.Image('JRC/GHSL/P2023A/GHS_POP/2025').select('population_count')

# Use AOI bounds for region

region = ee.Geometry.Rectangle([bbox[0], bbox[1], bbox[2], bbox[3]])

# Download as numpy array at 200m (still good resolution for population analysis)
pop_array = geemap.ee_to_numpy(
    pop_2025_ee,
    region=region,
    scale=200  # 200m to stay within 50MB API limit
)

# Squeeze out the band dimension
pop_2025 = np.squeeze(pop_array)

print(f"Population data shape: {pop_2025.shape}")
print(f"Resolution: 200m")
print(f"Min: {pop_2025.min():.2f}, Max: {pop_2025.max():.2f}")
print(f"Non-zero pixels: {np.count_nonzero(pop_2025 > 0):,}")
```


### Population and Flood Overlay

Compare the latest flood extent with population data to identify potential impacts.

```{python}
# Create a static visualization combining population and flood data
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

# Left panel: Population data
im1 = ax1.imshow(
    pop_2025,
    cmap='YlOrRd',
    interpolation='nearest',
    origin='upper',
    extent=[bbox[0], bbox[2], bbox[1], bbox[3]],
    vmin=0,
    vmax=200
)
gdf_aoi.boundary.plot(ax=ax1, color='black', linewidth=2, alpha=1.0)
ax1.set_title('Population Density 2025\n(people per grid cell)', fontsize=12, fontweight='bold')
ax1.set_xlabel('Longitude')
ax1.set_ylabel('Latitude')
plt.colorbar(im1, ax=ax1, label='Population Count')

# Right panel: Latest flood extent
data_flood = np.where(np.isnan(img_tlater), 255, img_tlater)
data_flood = data_flood[::8, ::8]

extent_flood = [
    img_tlater.x.min().item(),
    img_tlater.x.max().item(),
    img_tlater.y.min().item(),
    img_tlater.y.max().item(),
]

im2 = ax2.imshow(
    data_flood,
    cmap=cmap,
    norm=norm,
    interpolation='nearest',
    origin='upper',
    extent=extent_flood,
)
gdf_aoi.boundary.plot(ax=ax2, color='black', linewidth=2, alpha=1.0)
date_str = str(img_tlater.time.values)[:10]
ax2.set_title(f'Latest Flood Extent\n{date_str}', fontsize=12, fontweight='bold')
ax2.set_xlabel('Longitude')
ax2.set_ylabel('Latitude')

plt.tight_layout()
plt.show()
```
